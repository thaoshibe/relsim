{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.0408163265306123,
  "eval_steps": 500,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.004081632653061225,
      "grad_norm": 1.0066196918487549,
      "learning_rate": 0.0,
      "loss": 2.3937,
      "step": 1
    },
    {
      "epoch": 0.00816326530612245,
      "grad_norm": 0.8111227750778198,
      "learning_rate": 4.000000000000001e-06,
      "loss": 2.0672,
      "step": 2
    },
    {
      "epoch": 0.012244897959183673,
      "grad_norm": 0.8828960061073303,
      "learning_rate": 8.000000000000001e-06,
      "loss": 2.3789,
      "step": 3
    },
    {
      "epoch": 0.0163265306122449,
      "grad_norm": 0.9330424666404724,
      "learning_rate": 1.2e-05,
      "loss": 2.2966,
      "step": 4
    },
    {
      "epoch": 0.02040816326530612,
      "grad_norm": 0.933379054069519,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 2.314,
      "step": 5
    },
    {
      "epoch": 0.024489795918367346,
      "grad_norm": 0.9258625507354736,
      "learning_rate": 2e-05,
      "loss": 2.357,
      "step": 6
    },
    {
      "epoch": 0.02857142857142857,
      "grad_norm": 0.8572345972061157,
      "learning_rate": 2.4e-05,
      "loss": 2.2554,
      "step": 7
    },
    {
      "epoch": 0.0326530612244898,
      "grad_norm": 0.9033061265945435,
      "learning_rate": 2.8000000000000003e-05,
      "loss": 2.1706,
      "step": 8
    },
    {
      "epoch": 0.036734693877551024,
      "grad_norm": 0.9348958134651184,
      "learning_rate": 3.2000000000000005e-05,
      "loss": 2.4194,
      "step": 9
    },
    {
      "epoch": 0.04081632653061224,
      "grad_norm": 0.9678727984428406,
      "learning_rate": 3.6e-05,
      "loss": 2.122,
      "step": 10
    },
    {
      "epoch": 0.044897959183673466,
      "grad_norm": 0.9547260999679565,
      "learning_rate": 4e-05,
      "loss": 2.2501,
      "step": 11
    },
    {
      "epoch": 0.04897959183673469,
      "grad_norm": 0.8370831608772278,
      "learning_rate": 4.4000000000000006e-05,
      "loss": 2.1411,
      "step": 12
    },
    {
      "epoch": 0.053061224489795916,
      "grad_norm": 0.8208066821098328,
      "learning_rate": 4.8e-05,
      "loss": 1.9162,
      "step": 13
    },
    {
      "epoch": 0.05714285714285714,
      "grad_norm": 0.7401397824287415,
      "learning_rate": 5.2000000000000004e-05,
      "loss": 1.9299,
      "step": 14
    },
    {
      "epoch": 0.061224489795918366,
      "grad_norm": 0.8690797090530396,
      "learning_rate": 5.6000000000000006e-05,
      "loss": 2.0887,
      "step": 15
    },
    {
      "epoch": 0.0653061224489796,
      "grad_norm": 0.8780341148376465,
      "learning_rate": 6e-05,
      "loss": 1.9497,
      "step": 16
    },
    {
      "epoch": 0.06938775510204082,
      "grad_norm": 0.8367131948471069,
      "learning_rate": 6.400000000000001e-05,
      "loss": 2.0245,
      "step": 17
    },
    {
      "epoch": 0.07346938775510205,
      "grad_norm": 0.8914490342140198,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.0668,
      "step": 18
    },
    {
      "epoch": 0.07755102040816327,
      "grad_norm": 0.805556058883667,
      "learning_rate": 7.2e-05,
      "loss": 1.896,
      "step": 19
    },
    {
      "epoch": 0.08163265306122448,
      "grad_norm": 0.8404833674430847,
      "learning_rate": 7.6e-05,
      "loss": 1.9102,
      "step": 20
    },
    {
      "epoch": 0.08571428571428572,
      "grad_norm": 0.691921591758728,
      "learning_rate": 8e-05,
      "loss": 1.7254,
      "step": 21
    },
    {
      "epoch": 0.08979591836734693,
      "grad_norm": 0.8491175770759583,
      "learning_rate": 8.4e-05,
      "loss": 1.9476,
      "step": 22
    },
    {
      "epoch": 0.09387755102040816,
      "grad_norm": 0.7308470010757446,
      "learning_rate": 8.800000000000001e-05,
      "loss": 1.8029,
      "step": 23
    },
    {
      "epoch": 0.09795918367346938,
      "grad_norm": 0.9606391787528992,
      "learning_rate": 9.200000000000001e-05,
      "loss": 1.8644,
      "step": 24
    },
    {
      "epoch": 0.10204081632653061,
      "grad_norm": 1.0991623401641846,
      "learning_rate": 9.6e-05,
      "loss": 2.0638,
      "step": 25
    },
    {
      "epoch": 0.10612244897959183,
      "grad_norm": 1.0677579641342163,
      "learning_rate": 0.0001,
      "loss": 1.908,
      "step": 26
    },
    {
      "epoch": 0.11020408163265306,
      "grad_norm": 1.0215985774993896,
      "learning_rate": 0.00010400000000000001,
      "loss": 1.8813,
      "step": 27
    },
    {
      "epoch": 0.11428571428571428,
      "grad_norm": 0.832130491733551,
      "learning_rate": 0.00010800000000000001,
      "loss": 1.7078,
      "step": 28
    },
    {
      "epoch": 0.11836734693877551,
      "grad_norm": 0.7908713221549988,
      "learning_rate": 0.00011200000000000001,
      "loss": 1.6886,
      "step": 29
    },
    {
      "epoch": 0.12244897959183673,
      "grad_norm": 0.7467289566993713,
      "learning_rate": 0.000116,
      "loss": 1.7672,
      "step": 30
    },
    {
      "epoch": 0.12653061224489795,
      "grad_norm": 0.8515000343322754,
      "learning_rate": 0.00012,
      "loss": 1.8099,
      "step": 31
    },
    {
      "epoch": 0.1306122448979592,
      "grad_norm": 0.8727079629898071,
      "learning_rate": 0.000124,
      "loss": 1.8678,
      "step": 32
    },
    {
      "epoch": 0.1346938775510204,
      "grad_norm": 0.8256227374076843,
      "learning_rate": 0.00012800000000000002,
      "loss": 1.7989,
      "step": 33
    },
    {
      "epoch": 0.13877551020408163,
      "grad_norm": 0.7721722722053528,
      "learning_rate": 0.000132,
      "loss": 1.8282,
      "step": 34
    },
    {
      "epoch": 0.14285714285714285,
      "grad_norm": 0.805656909942627,
      "learning_rate": 0.00013600000000000003,
      "loss": 1.756,
      "step": 35
    },
    {
      "epoch": 0.1469387755102041,
      "grad_norm": 0.7526808977127075,
      "learning_rate": 0.00014,
      "loss": 1.7226,
      "step": 36
    },
    {
      "epoch": 0.1510204081632653,
      "grad_norm": 0.7861364483833313,
      "learning_rate": 0.000144,
      "loss": 1.7058,
      "step": 37
    },
    {
      "epoch": 0.15510204081632653,
      "grad_norm": 0.6913377046585083,
      "learning_rate": 0.000148,
      "loss": 1.7594,
      "step": 38
    },
    {
      "epoch": 0.15918367346938775,
      "grad_norm": 0.7342740297317505,
      "learning_rate": 0.000152,
      "loss": 1.6648,
      "step": 39
    },
    {
      "epoch": 0.16326530612244897,
      "grad_norm": 0.760233461856842,
      "learning_rate": 0.00015600000000000002,
      "loss": 1.7683,
      "step": 40
    },
    {
      "epoch": 0.1673469387755102,
      "grad_norm": 0.7360990047454834,
      "learning_rate": 0.00016,
      "loss": 1.6806,
      "step": 41
    },
    {
      "epoch": 0.17142857142857143,
      "grad_norm": 0.635276734828949,
      "learning_rate": 0.000164,
      "loss": 1.5571,
      "step": 42
    },
    {
      "epoch": 0.17551020408163265,
      "grad_norm": 0.6379397511482239,
      "learning_rate": 0.000168,
      "loss": 1.7109,
      "step": 43
    },
    {
      "epoch": 0.17959183673469387,
      "grad_norm": 0.5801832675933838,
      "learning_rate": 0.000172,
      "loss": 1.6678,
      "step": 44
    },
    {
      "epoch": 0.1836734693877551,
      "grad_norm": 0.681705892086029,
      "learning_rate": 0.00017600000000000002,
      "loss": 1.6517,
      "step": 45
    },
    {
      "epoch": 0.18775510204081633,
      "grad_norm": 0.8721517324447632,
      "learning_rate": 0.00018,
      "loss": 1.6831,
      "step": 46
    },
    {
      "epoch": 0.19183673469387755,
      "grad_norm": 0.9912824034690857,
      "learning_rate": 0.00018400000000000003,
      "loss": 1.6147,
      "step": 47
    },
    {
      "epoch": 0.19591836734693877,
      "grad_norm": 0.7370397448539734,
      "learning_rate": 0.000188,
      "loss": 1.6569,
      "step": 48
    },
    {
      "epoch": 0.2,
      "grad_norm": 1.0094512701034546,
      "learning_rate": 0.000192,
      "loss": 1.6462,
      "step": 49
    },
    {
      "epoch": 0.20408163265306123,
      "grad_norm": 0.8065770864486694,
      "learning_rate": 0.000196,
      "loss": 1.5527,
      "step": 50
    },
    {
      "epoch": 0.20816326530612245,
      "grad_norm": 0.8927117586135864,
      "learning_rate": 0.0002,
      "loss": 1.5962,
      "step": 51
    },
    {
      "epoch": 0.21224489795918366,
      "grad_norm": 0.7092443704605103,
      "learning_rate": 0.00019995876288659794,
      "loss": 1.5921,
      "step": 52
    },
    {
      "epoch": 0.2163265306122449,
      "grad_norm": 0.7708242535591125,
      "learning_rate": 0.0001999175257731959,
      "loss": 1.5989,
      "step": 53
    },
    {
      "epoch": 0.22040816326530613,
      "grad_norm": 0.8990233540534973,
      "learning_rate": 0.00019987628865979383,
      "loss": 1.5768,
      "step": 54
    },
    {
      "epoch": 0.22448979591836735,
      "grad_norm": 0.7708848714828491,
      "learning_rate": 0.00019983505154639176,
      "loss": 1.4443,
      "step": 55
    },
    {
      "epoch": 0.22857142857142856,
      "grad_norm": 0.7996214628219604,
      "learning_rate": 0.0001997938144329897,
      "loss": 1.4814,
      "step": 56
    },
    {
      "epoch": 0.23265306122448978,
      "grad_norm": 0.8266173005104065,
      "learning_rate": 0.00019975257731958762,
      "loss": 1.5861,
      "step": 57
    },
    {
      "epoch": 0.23673469387755103,
      "grad_norm": 0.9371815323829651,
      "learning_rate": 0.00019971134020618558,
      "loss": 1.5961,
      "step": 58
    },
    {
      "epoch": 0.24081632653061225,
      "grad_norm": 0.7878872156143188,
      "learning_rate": 0.00019967010309278351,
      "loss": 1.5273,
      "step": 59
    },
    {
      "epoch": 0.24489795918367346,
      "grad_norm": 0.7182327508926392,
      "learning_rate": 0.00019962886597938147,
      "loss": 1.5165,
      "step": 60
    },
    {
      "epoch": 0.24897959183673468,
      "grad_norm": 0.7820907831192017,
      "learning_rate": 0.0001995876288659794,
      "loss": 1.4511,
      "step": 61
    },
    {
      "epoch": 0.2530612244897959,
      "grad_norm": 0.9566872715950012,
      "learning_rate": 0.00019954639175257733,
      "loss": 1.6597,
      "step": 62
    },
    {
      "epoch": 0.2571428571428571,
      "grad_norm": 1.0003091096878052,
      "learning_rate": 0.00019950515463917527,
      "loss": 1.7076,
      "step": 63
    },
    {
      "epoch": 0.2612244897959184,
      "grad_norm": 0.8265464901924133,
      "learning_rate": 0.0001994639175257732,
      "loss": 1.4668,
      "step": 64
    },
    {
      "epoch": 0.2653061224489796,
      "grad_norm": 0.7457850575447083,
      "learning_rate": 0.00019942268041237116,
      "loss": 1.4811,
      "step": 65
    },
    {
      "epoch": 0.2693877551020408,
      "grad_norm": 0.9266958832740784,
      "learning_rate": 0.0001993814432989691,
      "loss": 1.5778,
      "step": 66
    },
    {
      "epoch": 0.27346938775510204,
      "grad_norm": 1.0013591051101685,
      "learning_rate": 0.00019934020618556702,
      "loss": 1.4301,
      "step": 67
    },
    {
      "epoch": 0.27755102040816326,
      "grad_norm": 0.7251825928688049,
      "learning_rate": 0.00019929896907216498,
      "loss": 1.4275,
      "step": 68
    },
    {
      "epoch": 0.2816326530612245,
      "grad_norm": 0.7609503865242004,
      "learning_rate": 0.00019925773195876288,
      "loss": 1.5659,
      "step": 69
    },
    {
      "epoch": 0.2857142857142857,
      "grad_norm": 0.7951069474220276,
      "learning_rate": 0.0001992164948453608,
      "loss": 1.5078,
      "step": 70
    },
    {
      "epoch": 0.2897959183673469,
      "grad_norm": 0.918237566947937,
      "learning_rate": 0.00019917525773195877,
      "loss": 1.4148,
      "step": 71
    },
    {
      "epoch": 0.2938775510204082,
      "grad_norm": 0.8701842427253723,
      "learning_rate": 0.0001991340206185567,
      "loss": 1.5262,
      "step": 72
    },
    {
      "epoch": 0.2979591836734694,
      "grad_norm": 0.8709402680397034,
      "learning_rate": 0.00019909278350515466,
      "loss": 1.4497,
      "step": 73
    },
    {
      "epoch": 0.3020408163265306,
      "grad_norm": 1.0966330766677856,
      "learning_rate": 0.0001990515463917526,
      "loss": 1.4259,
      "step": 74
    },
    {
      "epoch": 0.30612244897959184,
      "grad_norm": 1.0014147758483887,
      "learning_rate": 0.00019901030927835052,
      "loss": 1.4723,
      "step": 75
    },
    {
      "epoch": 0.31020408163265306,
      "grad_norm": 1.0854867696762085,
      "learning_rate": 0.00019896907216494845,
      "loss": 1.3899,
      "step": 76
    },
    {
      "epoch": 0.3142857142857143,
      "grad_norm": 1.0055208206176758,
      "learning_rate": 0.00019892783505154639,
      "loss": 1.462,
      "step": 77
    },
    {
      "epoch": 0.3183673469387755,
      "grad_norm": 1.0767091512680054,
      "learning_rate": 0.00019888659793814434,
      "loss": 1.3953,
      "step": 78
    },
    {
      "epoch": 0.3224489795918367,
      "grad_norm": 1.1456220149993896,
      "learning_rate": 0.00019884536082474227,
      "loss": 1.5145,
      "step": 79
    },
    {
      "epoch": 0.32653061224489793,
      "grad_norm": 0.9288462996482849,
      "learning_rate": 0.0001988041237113402,
      "loss": 1.4892,
      "step": 80
    },
    {
      "epoch": 0.3306122448979592,
      "grad_norm": 0.9128601551055908,
      "learning_rate": 0.00019876288659793816,
      "loss": 1.5025,
      "step": 81
    },
    {
      "epoch": 0.3346938775510204,
      "grad_norm": 1.1154123544692993,
      "learning_rate": 0.0001987216494845361,
      "loss": 1.4818,
      "step": 82
    },
    {
      "epoch": 0.33877551020408164,
      "grad_norm": 0.8443418741226196,
      "learning_rate": 0.00019868041237113403,
      "loss": 1.4291,
      "step": 83
    },
    {
      "epoch": 0.34285714285714286,
      "grad_norm": 0.8660030364990234,
      "learning_rate": 0.00019863917525773196,
      "loss": 1.3789,
      "step": 84
    },
    {
      "epoch": 0.3469387755102041,
      "grad_norm": 1.0583438873291016,
      "learning_rate": 0.0001985979381443299,
      "loss": 1.503,
      "step": 85
    },
    {
      "epoch": 0.3510204081632653,
      "grad_norm": 0.9142259359359741,
      "learning_rate": 0.00019855670103092785,
      "loss": 1.4564,
      "step": 86
    },
    {
      "epoch": 0.3551020408163265,
      "grad_norm": 1.0187463760375977,
      "learning_rate": 0.00019851546391752578,
      "loss": 1.3495,
      "step": 87
    },
    {
      "epoch": 0.35918367346938773,
      "grad_norm": 1.1203224658966064,
      "learning_rate": 0.00019847422680412374,
      "loss": 1.4837,
      "step": 88
    },
    {
      "epoch": 0.363265306122449,
      "grad_norm": 1.0427024364471436,
      "learning_rate": 0.00019843298969072167,
      "loss": 1.4263,
      "step": 89
    },
    {
      "epoch": 0.3673469387755102,
      "grad_norm": 1.0579665899276733,
      "learning_rate": 0.0001983917525773196,
      "loss": 1.471,
      "step": 90
    },
    {
      "epoch": 0.37142857142857144,
      "grad_norm": 1.1313790082931519,
      "learning_rate": 0.00019835051546391753,
      "loss": 1.4649,
      "step": 91
    },
    {
      "epoch": 0.37551020408163266,
      "grad_norm": 0.9363969564437866,
      "learning_rate": 0.00019830927835051546,
      "loss": 1.4105,
      "step": 92
    },
    {
      "epoch": 0.3795918367346939,
      "grad_norm": 1.0996441841125488,
      "learning_rate": 0.00019826804123711342,
      "loss": 1.359,
      "step": 93
    },
    {
      "epoch": 0.3836734693877551,
      "grad_norm": 1.1246479749679565,
      "learning_rate": 0.00019822680412371135,
      "loss": 1.5727,
      "step": 94
    },
    {
      "epoch": 0.3877551020408163,
      "grad_norm": 1.013669729232788,
      "learning_rate": 0.00019818556701030928,
      "loss": 1.4833,
      "step": 95
    },
    {
      "epoch": 0.39183673469387753,
      "grad_norm": 1.0487631559371948,
      "learning_rate": 0.00019814432989690724,
      "loss": 1.4794,
      "step": 96
    },
    {
      "epoch": 0.39591836734693875,
      "grad_norm": 0.9332361817359924,
      "learning_rate": 0.00019810309278350517,
      "loss": 1.3061,
      "step": 97
    },
    {
      "epoch": 0.4,
      "grad_norm": 1.0051308870315552,
      "learning_rate": 0.0001980618556701031,
      "loss": 1.2762,
      "step": 98
    },
    {
      "epoch": 0.40408163265306124,
      "grad_norm": 1.0423808097839355,
      "learning_rate": 0.00019802061855670104,
      "loss": 1.3682,
      "step": 99
    },
    {
      "epoch": 0.40816326530612246,
      "grad_norm": 0.9869229793548584,
      "learning_rate": 0.00019797938144329897,
      "loss": 1.4054,
      "step": 100
    },
    {
      "epoch": 0.4122448979591837,
      "grad_norm": 1.0072479248046875,
      "learning_rate": 0.00019793814432989693,
      "loss": 1.4425,
      "step": 101
    },
    {
      "epoch": 0.4163265306122449,
      "grad_norm": 0.9978590607643127,
      "learning_rate": 0.00019789690721649486,
      "loss": 1.2002,
      "step": 102
    },
    {
      "epoch": 0.4204081632653061,
      "grad_norm": 1.1069085597991943,
      "learning_rate": 0.0001978556701030928,
      "loss": 1.3774,
      "step": 103
    },
    {
      "epoch": 0.42448979591836733,
      "grad_norm": 1.128228783607483,
      "learning_rate": 0.00019781443298969075,
      "loss": 1.4305,
      "step": 104
    },
    {
      "epoch": 0.42857142857142855,
      "grad_norm": 1.2758393287658691,
      "learning_rate": 0.00019777319587628865,
      "loss": 1.4406,
      "step": 105
    },
    {
      "epoch": 0.4326530612244898,
      "grad_norm": 1.4047893285751343,
      "learning_rate": 0.0001977319587628866,
      "loss": 1.4571,
      "step": 106
    },
    {
      "epoch": 0.43673469387755104,
      "grad_norm": 1.2083461284637451,
      "learning_rate": 0.00019769072164948454,
      "loss": 1.5102,
      "step": 107
    },
    {
      "epoch": 0.44081632653061226,
      "grad_norm": 0.9923112392425537,
      "learning_rate": 0.00019764948453608247,
      "loss": 1.2524,
      "step": 108
    },
    {
      "epoch": 0.4448979591836735,
      "grad_norm": 1.0450313091278076,
      "learning_rate": 0.00019760824742268043,
      "loss": 1.3225,
      "step": 109
    },
    {
      "epoch": 0.4489795918367347,
      "grad_norm": 1.2071278095245361,
      "learning_rate": 0.00019756701030927836,
      "loss": 1.3286,
      "step": 110
    },
    {
      "epoch": 0.4530612244897959,
      "grad_norm": 1.23637855052948,
      "learning_rate": 0.00019752577319587632,
      "loss": 1.4137,
      "step": 111
    },
    {
      "epoch": 0.45714285714285713,
      "grad_norm": 1.0211502313613892,
      "learning_rate": 0.00019748453608247422,
      "loss": 1.384,
      "step": 112
    },
    {
      "epoch": 0.46122448979591835,
      "grad_norm": 1.052612066268921,
      "learning_rate": 0.00019744329896907216,
      "loss": 1.239,
      "step": 113
    },
    {
      "epoch": 0.46530612244897956,
      "grad_norm": 1.1411033868789673,
      "learning_rate": 0.0001974020618556701,
      "loss": 1.2852,
      "step": 114
    },
    {
      "epoch": 0.46938775510204084,
      "grad_norm": 1.1846424341201782,
      "learning_rate": 0.00019736082474226804,
      "loss": 1.3911,
      "step": 115
    },
    {
      "epoch": 0.47346938775510206,
      "grad_norm": 1.3087859153747559,
      "learning_rate": 0.000197319587628866,
      "loss": 1.2421,
      "step": 116
    },
    {
      "epoch": 0.4775510204081633,
      "grad_norm": 1.00858736038208,
      "learning_rate": 0.00019727835051546393,
      "loss": 1.1619,
      "step": 117
    },
    {
      "epoch": 0.4816326530612245,
      "grad_norm": 1.2539231777191162,
      "learning_rate": 0.00019723711340206187,
      "loss": 1.2312,
      "step": 118
    },
    {
      "epoch": 0.4857142857142857,
      "grad_norm": 1.136009693145752,
      "learning_rate": 0.0001971958762886598,
      "loss": 1.4599,
      "step": 119
    },
    {
      "epoch": 0.4897959183673469,
      "grad_norm": 1.3831442594528198,
      "learning_rate": 0.00019715463917525773,
      "loss": 1.4584,
      "step": 120
    },
    {
      "epoch": 0.49387755102040815,
      "grad_norm": 1.2995251417160034,
      "learning_rate": 0.0001971134020618557,
      "loss": 1.1999,
      "step": 121
    },
    {
      "epoch": 0.49795918367346936,
      "grad_norm": 1.214924931526184,
      "learning_rate": 0.00019707216494845362,
      "loss": 1.2795,
      "step": 122
    },
    {
      "epoch": 0.5020408163265306,
      "grad_norm": 1.3703854084014893,
      "learning_rate": 0.00019703092783505155,
      "loss": 1.3605,
      "step": 123
    },
    {
      "epoch": 0.5061224489795918,
      "grad_norm": 1.5710703134536743,
      "learning_rate": 0.0001969896907216495,
      "loss": 1.3472,
      "step": 124
    },
    {
      "epoch": 0.5102040816326531,
      "grad_norm": 1.3603105545043945,
      "learning_rate": 0.00019694845360824744,
      "loss": 1.2534,
      "step": 125
    },
    {
      "epoch": 0.5142857142857142,
      "grad_norm": 1.089582085609436,
      "learning_rate": 0.00019690721649484537,
      "loss": 1.1933,
      "step": 126
    },
    {
      "epoch": 0.5183673469387755,
      "grad_norm": 1.3573743104934692,
      "learning_rate": 0.0001968659793814433,
      "loss": 1.2903,
      "step": 127
    },
    {
      "epoch": 0.5224489795918368,
      "grad_norm": 1.3988687992095947,
      "learning_rate": 0.00019682474226804123,
      "loss": 1.2876,
      "step": 128
    },
    {
      "epoch": 0.5265306122448979,
      "grad_norm": 1.4072086811065674,
      "learning_rate": 0.0001967835051546392,
      "loss": 1.3712,
      "step": 129
    },
    {
      "epoch": 0.5306122448979592,
      "grad_norm": 1.454488754272461,
      "learning_rate": 0.00019674226804123712,
      "loss": 1.3936,
      "step": 130
    },
    {
      "epoch": 0.5346938775510204,
      "grad_norm": 1.1702677011489868,
      "learning_rate": 0.00019670103092783505,
      "loss": 1.2891,
      "step": 131
    },
    {
      "epoch": 0.5387755102040817,
      "grad_norm": 1.6716188192367554,
      "learning_rate": 0.000196659793814433,
      "loss": 1.4312,
      "step": 132
    },
    {
      "epoch": 0.5428571428571428,
      "grad_norm": 1.535500168800354,
      "learning_rate": 0.00019661855670103094,
      "loss": 1.3614,
      "step": 133
    },
    {
      "epoch": 0.5469387755102041,
      "grad_norm": 0.9980366826057434,
      "learning_rate": 0.00019657731958762887,
      "loss": 1.1471,
      "step": 134
    },
    {
      "epoch": 0.5510204081632653,
      "grad_norm": 1.1967047452926636,
      "learning_rate": 0.0001965360824742268,
      "loss": 1.2321,
      "step": 135
    },
    {
      "epoch": 0.5551020408163265,
      "grad_norm": 1.2445038557052612,
      "learning_rate": 0.00019649484536082474,
      "loss": 1.2888,
      "step": 136
    },
    {
      "epoch": 0.5591836734693878,
      "grad_norm": 1.2963111400604248,
      "learning_rate": 0.0001964536082474227,
      "loss": 1.1412,
      "step": 137
    },
    {
      "epoch": 0.563265306122449,
      "grad_norm": 1.2623040676116943,
      "learning_rate": 0.00019641237113402063,
      "loss": 1.2561,
      "step": 138
    },
    {
      "epoch": 0.5673469387755102,
      "grad_norm": 1.3925987482070923,
      "learning_rate": 0.00019637113402061859,
      "loss": 1.3101,
      "step": 139
    },
    {
      "epoch": 0.5714285714285714,
      "grad_norm": 1.5252203941345215,
      "learning_rate": 0.00019632989690721652,
      "loss": 1.2226,
      "step": 140
    },
    {
      "epoch": 0.5755102040816327,
      "grad_norm": 1.2675552368164062,
      "learning_rate": 0.00019628865979381442,
      "loss": 1.2731,
      "step": 141
    },
    {
      "epoch": 0.5795918367346938,
      "grad_norm": 1.4104350805282593,
      "learning_rate": 0.00019624742268041238,
      "loss": 1.1709,
      "step": 142
    },
    {
      "epoch": 0.5836734693877551,
      "grad_norm": 1.4457120895385742,
      "learning_rate": 0.0001962061855670103,
      "loss": 1.2294,
      "step": 143
    },
    {
      "epoch": 0.5877551020408164,
      "grad_norm": 1.1845941543579102,
      "learning_rate": 0.00019616494845360827,
      "loss": 1.1628,
      "step": 144
    },
    {
      "epoch": 0.5918367346938775,
      "grad_norm": 1.4211854934692383,
      "learning_rate": 0.0001961237113402062,
      "loss": 1.1621,
      "step": 145
    },
    {
      "epoch": 0.5959183673469388,
      "grad_norm": 1.321496605873108,
      "learning_rate": 0.00019608247422680413,
      "loss": 1.2077,
      "step": 146
    },
    {
      "epoch": 0.6,
      "grad_norm": 1.440840482711792,
      "learning_rate": 0.0001960412371134021,
      "loss": 1.1911,
      "step": 147
    },
    {
      "epoch": 0.6040816326530613,
      "grad_norm": 1.4550305604934692,
      "learning_rate": 0.000196,
      "loss": 1.3768,
      "step": 148
    },
    {
      "epoch": 0.6081632653061224,
      "grad_norm": 1.4076696634292603,
      "learning_rate": 0.00019595876288659795,
      "loss": 1.3588,
      "step": 149
    },
    {
      "epoch": 0.6122448979591837,
      "grad_norm": 1.5208392143249512,
      "learning_rate": 0.00019591752577319588,
      "loss": 1.2512,
      "step": 150
    },
    {
      "epoch": 0.6163265306122448,
      "grad_norm": 1.631460428237915,
      "learning_rate": 0.00019587628865979381,
      "loss": 1.2256,
      "step": 151
    },
    {
      "epoch": 0.6204081632653061,
      "grad_norm": 1.5686885118484497,
      "learning_rate": 0.00019583505154639177,
      "loss": 1.4924,
      "step": 152
    },
    {
      "epoch": 0.6244897959183674,
      "grad_norm": 1.2956994771957397,
      "learning_rate": 0.0001957938144329897,
      "loss": 1.2157,
      "step": 153
    },
    {
      "epoch": 0.6285714285714286,
      "grad_norm": 1.1814130544662476,
      "learning_rate": 0.00019575257731958764,
      "loss": 1.1829,
      "step": 154
    },
    {
      "epoch": 0.6326530612244898,
      "grad_norm": 1.3749762773513794,
      "learning_rate": 0.00019571134020618557,
      "loss": 1.2518,
      "step": 155
    },
    {
      "epoch": 0.636734693877551,
      "grad_norm": 1.2398476600646973,
      "learning_rate": 0.0001956701030927835,
      "loss": 1.1135,
      "step": 156
    },
    {
      "epoch": 0.6408163265306123,
      "grad_norm": 1.343010425567627,
      "learning_rate": 0.00019562886597938146,
      "loss": 1.1538,
      "step": 157
    },
    {
      "epoch": 0.6448979591836734,
      "grad_norm": 1.2256923913955688,
      "learning_rate": 0.0001955876288659794,
      "loss": 1.0546,
      "step": 158
    },
    {
      "epoch": 0.6489795918367347,
      "grad_norm": 1.3378232717514038,
      "learning_rate": 0.00019554639175257732,
      "loss": 1.214,
      "step": 159
    },
    {
      "epoch": 0.6530612244897959,
      "grad_norm": 1.3589295148849487,
      "learning_rate": 0.00019550515463917528,
      "loss": 1.2712,
      "step": 160
    },
    {
      "epoch": 0.6571428571428571,
      "grad_norm": 1.3435256481170654,
      "learning_rate": 0.0001954639175257732,
      "loss": 1.1746,
      "step": 161
    },
    {
      "epoch": 0.6612244897959184,
      "grad_norm": 1.3953744173049927,
      "learning_rate": 0.00019542268041237114,
      "loss": 1.1756,
      "step": 162
    },
    {
      "epoch": 0.6653061224489796,
      "grad_norm": 1.3355488777160645,
      "learning_rate": 0.00019538144329896907,
      "loss": 1.1558,
      "step": 163
    },
    {
      "epoch": 0.6693877551020408,
      "grad_norm": 1.3901647329330444,
      "learning_rate": 0.000195340206185567,
      "loss": 1.1969,
      "step": 164
    },
    {
      "epoch": 0.673469387755102,
      "grad_norm": 1.2692614793777466,
      "learning_rate": 0.00019529896907216496,
      "loss": 1.1304,
      "step": 165
    },
    {
      "epoch": 0.6775510204081633,
      "grad_norm": 1.3619868755340576,
      "learning_rate": 0.0001952577319587629,
      "loss": 1.0998,
      "step": 166
    },
    {
      "epoch": 0.6816326530612244,
      "grad_norm": 1.1986775398254395,
      "learning_rate": 0.00019521649484536085,
      "loss": 1.2179,
      "step": 167
    },
    {
      "epoch": 0.6857142857142857,
      "grad_norm": 1.8305829763412476,
      "learning_rate": 0.00019517525773195878,
      "loss": 1.2056,
      "step": 168
    },
    {
      "epoch": 0.689795918367347,
      "grad_norm": 1.348913550376892,
      "learning_rate": 0.0001951340206185567,
      "loss": 1.1445,
      "step": 169
    },
    {
      "epoch": 0.6938775510204082,
      "grad_norm": 1.3086771965026855,
      "learning_rate": 0.00019509278350515464,
      "loss": 1.1093,
      "step": 170
    },
    {
      "epoch": 0.6979591836734694,
      "grad_norm": 1.3929176330566406,
      "learning_rate": 0.00019505154639175258,
      "loss": 1.2182,
      "step": 171
    },
    {
      "epoch": 0.7020408163265306,
      "grad_norm": 1.4015275239944458,
      "learning_rate": 0.00019501030927835053,
      "loss": 1.1154,
      "step": 172
    },
    {
      "epoch": 0.7061224489795919,
      "grad_norm": 1.4884090423583984,
      "learning_rate": 0.00019496907216494847,
      "loss": 1.2083,
      "step": 173
    },
    {
      "epoch": 0.710204081632653,
      "grad_norm": 1.542428970336914,
      "learning_rate": 0.0001949278350515464,
      "loss": 1.0412,
      "step": 174
    },
    {
      "epoch": 0.7142857142857143,
      "grad_norm": 1.623184084892273,
      "learning_rate": 0.00019488659793814435,
      "loss": 1.23,
      "step": 175
    },
    {
      "epoch": 0.7183673469387755,
      "grad_norm": 1.6110332012176514,
      "learning_rate": 0.00019484536082474229,
      "loss": 1.3452,
      "step": 176
    },
    {
      "epoch": 0.7224489795918367,
      "grad_norm": 1.4758654832839966,
      "learning_rate": 0.00019480412371134022,
      "loss": 1.0375,
      "step": 177
    },
    {
      "epoch": 0.726530612244898,
      "grad_norm": 1.3649382591247559,
      "learning_rate": 0.00019476288659793815,
      "loss": 1.2318,
      "step": 178
    },
    {
      "epoch": 0.7306122448979592,
      "grad_norm": 1.5865687131881714,
      "learning_rate": 0.00019472164948453608,
      "loss": 1.2021,
      "step": 179
    },
    {
      "epoch": 0.7346938775510204,
      "grad_norm": 1.3510140180587769,
      "learning_rate": 0.00019468041237113404,
      "loss": 1.165,
      "step": 180
    },
    {
      "epoch": 0.7387755102040816,
      "grad_norm": 1.4622968435287476,
      "learning_rate": 0.00019463917525773197,
      "loss": 1.2769,
      "step": 181
    },
    {
      "epoch": 0.7428571428571429,
      "grad_norm": 1.4747719764709473,
      "learning_rate": 0.00019459793814432993,
      "loss": 1.0321,
      "step": 182
    },
    {
      "epoch": 0.746938775510204,
      "grad_norm": 1.63629949092865,
      "learning_rate": 0.00019455670103092786,
      "loss": 1.1855,
      "step": 183
    },
    {
      "epoch": 0.7510204081632653,
      "grad_norm": 1.5672305822372437,
      "learning_rate": 0.00019451546391752576,
      "loss": 1.0099,
      "step": 184
    },
    {
      "epoch": 0.7551020408163265,
      "grad_norm": 1.4850996732711792,
      "learning_rate": 0.00019447422680412372,
      "loss": 1.0741,
      "step": 185
    },
    {
      "epoch": 0.7591836734693878,
      "grad_norm": 1.5715667009353638,
      "learning_rate": 0.00019443298969072165,
      "loss": 1.0662,
      "step": 186
    },
    {
      "epoch": 0.763265306122449,
      "grad_norm": 1.5132787227630615,
      "learning_rate": 0.0001943917525773196,
      "loss": 1.1825,
      "step": 187
    },
    {
      "epoch": 0.7673469387755102,
      "grad_norm": 1.39919912815094,
      "learning_rate": 0.00019435051546391754,
      "loss": 1.0497,
      "step": 188
    },
    {
      "epoch": 0.7714285714285715,
      "grad_norm": 1.656277060508728,
      "learning_rate": 0.00019430927835051547,
      "loss": 1.2607,
      "step": 189
    },
    {
      "epoch": 0.7755102040816326,
      "grad_norm": 1.8009397983551025,
      "learning_rate": 0.0001942680412371134,
      "loss": 1.2731,
      "step": 190
    },
    {
      "epoch": 0.7795918367346939,
      "grad_norm": 1.8341562747955322,
      "learning_rate": 0.00019422680412371134,
      "loss": 1.2455,
      "step": 191
    },
    {
      "epoch": 0.7836734693877551,
      "grad_norm": 1.4572381973266602,
      "learning_rate": 0.00019418556701030927,
      "loss": 1.0911,
      "step": 192
    },
    {
      "epoch": 0.7877551020408163,
      "grad_norm": 1.6291018724441528,
      "learning_rate": 0.00019414432989690723,
      "loss": 1.2303,
      "step": 193
    },
    {
      "epoch": 0.7918367346938775,
      "grad_norm": 1.3687458038330078,
      "learning_rate": 0.00019410309278350516,
      "loss": 1.2173,
      "step": 194
    },
    {
      "epoch": 0.7959183673469388,
      "grad_norm": 1.6020030975341797,
      "learning_rate": 0.00019406185567010312,
      "loss": 1.2943,
      "step": 195
    },
    {
      "epoch": 0.8,
      "grad_norm": 1.630435824394226,
      "learning_rate": 0.00019402061855670105,
      "loss": 1.1016,
      "step": 196
    },
    {
      "epoch": 0.8040816326530612,
      "grad_norm": 1.5288538932800293,
      "learning_rate": 0.00019397938144329898,
      "loss": 1.0657,
      "step": 197
    },
    {
      "epoch": 0.8081632653061225,
      "grad_norm": 1.4423998594284058,
      "learning_rate": 0.0001939381443298969,
      "loss": 1.0725,
      "step": 198
    },
    {
      "epoch": 0.8122448979591836,
      "grad_norm": 1.523980975151062,
      "learning_rate": 0.00019389690721649484,
      "loss": 1.0436,
      "step": 199
    },
    {
      "epoch": 0.8163265306122449,
      "grad_norm": 1.6922616958618164,
      "learning_rate": 0.0001938556701030928,
      "loss": 1.2912,
      "step": 200
    },
    {
      "epoch": 0.8204081632653061,
      "grad_norm": 1.5204296112060547,
      "learning_rate": 0.00019381443298969073,
      "loss": 1.1234,
      "step": 201
    },
    {
      "epoch": 0.8244897959183674,
      "grad_norm": 1.3610987663269043,
      "learning_rate": 0.00019377319587628866,
      "loss": 1.002,
      "step": 202
    },
    {
      "epoch": 0.8285714285714286,
      "grad_norm": 1.7361421585083008,
      "learning_rate": 0.00019373195876288662,
      "loss": 1.0701,
      "step": 203
    },
    {
      "epoch": 0.8326530612244898,
      "grad_norm": 1.5438382625579834,
      "learning_rate": 0.00019369072164948455,
      "loss": 1.1068,
      "step": 204
    },
    {
      "epoch": 0.8367346938775511,
      "grad_norm": 1.4989583492279053,
      "learning_rate": 0.00019364948453608248,
      "loss": 0.8725,
      "step": 205
    },
    {
      "epoch": 0.8408163265306122,
      "grad_norm": 1.8776763677597046,
      "learning_rate": 0.00019360824742268041,
      "loss": 1.1195,
      "step": 206
    },
    {
      "epoch": 0.8448979591836735,
      "grad_norm": 1.6096696853637695,
      "learning_rate": 0.00019356701030927835,
      "loss": 1.0552,
      "step": 207
    },
    {
      "epoch": 0.8489795918367347,
      "grad_norm": 1.7352017164230347,
      "learning_rate": 0.0001935257731958763,
      "loss": 1.17,
      "step": 208
    },
    {
      "epoch": 0.8530612244897959,
      "grad_norm": 1.7988940477371216,
      "learning_rate": 0.00019348453608247424,
      "loss": 1.1704,
      "step": 209
    },
    {
      "epoch": 0.8571428571428571,
      "grad_norm": 1.6610277891159058,
      "learning_rate": 0.0001934432989690722,
      "loss": 1.0971,
      "step": 210
    },
    {
      "epoch": 0.8612244897959184,
      "grad_norm": 1.7854876518249512,
      "learning_rate": 0.00019340206185567012,
      "loss": 1.055,
      "step": 211
    },
    {
      "epoch": 0.8653061224489796,
      "grad_norm": 1.4257512092590332,
      "learning_rate": 0.00019336082474226806,
      "loss": 0.9223,
      "step": 212
    },
    {
      "epoch": 0.8693877551020408,
      "grad_norm": 1.8188683986663818,
      "learning_rate": 0.000193319587628866,
      "loss": 1.1972,
      "step": 213
    },
    {
      "epoch": 0.8734693877551021,
      "grad_norm": 1.600361943244934,
      "learning_rate": 0.00019327835051546392,
      "loss": 1.0672,
      "step": 214
    },
    {
      "epoch": 0.8775510204081632,
      "grad_norm": 1.5897703170776367,
      "learning_rate": 0.00019323711340206188,
      "loss": 1.0757,
      "step": 215
    },
    {
      "epoch": 0.8816326530612245,
      "grad_norm": 1.621647834777832,
      "learning_rate": 0.0001931958762886598,
      "loss": 1.0687,
      "step": 216
    },
    {
      "epoch": 0.8857142857142857,
      "grad_norm": 1.3291243314743042,
      "learning_rate": 0.00019315463917525774,
      "loss": 0.9149,
      "step": 217
    },
    {
      "epoch": 0.889795918367347,
      "grad_norm": 1.5063893795013428,
      "learning_rate": 0.0001931134020618557,
      "loss": 1.1972,
      "step": 218
    },
    {
      "epoch": 0.8938775510204081,
      "grad_norm": 1.4484188556671143,
      "learning_rate": 0.0001930721649484536,
      "loss": 0.9713,
      "step": 219
    },
    {
      "epoch": 0.8979591836734694,
      "grad_norm": 1.6771066188812256,
      "learning_rate": 0.00019303092783505153,
      "loss": 1.1289,
      "step": 220
    },
    {
      "epoch": 0.9020408163265307,
      "grad_norm": 1.7333083152770996,
      "learning_rate": 0.0001929896907216495,
      "loss": 1.1498,
      "step": 221
    },
    {
      "epoch": 0.9061224489795918,
      "grad_norm": 1.4366198778152466,
      "learning_rate": 0.00019294845360824742,
      "loss": 0.7959,
      "step": 222
    },
    {
      "epoch": 0.9102040816326531,
      "grad_norm": 1.5725635290145874,
      "learning_rate": 0.00019290721649484538,
      "loss": 1.1719,
      "step": 223
    },
    {
      "epoch": 0.9142857142857143,
      "grad_norm": 1.352971076965332,
      "learning_rate": 0.0001928659793814433,
      "loss": 0.9402,
      "step": 224
    },
    {
      "epoch": 0.9183673469387755,
      "grad_norm": 1.5377901792526245,
      "learning_rate": 0.00019282474226804124,
      "loss": 1.0636,
      "step": 225
    },
    {
      "epoch": 0.9224489795918367,
      "grad_norm": 1.611014723777771,
      "learning_rate": 0.00019278350515463918,
      "loss": 1.0281,
      "step": 226
    },
    {
      "epoch": 0.926530612244898,
      "grad_norm": 1.798622727394104,
      "learning_rate": 0.0001927422680412371,
      "loss": 1.0266,
      "step": 227
    },
    {
      "epoch": 0.9306122448979591,
      "grad_norm": 1.6902672052383423,
      "learning_rate": 0.00019270103092783506,
      "loss": 1.128,
      "step": 228
    },
    {
      "epoch": 0.9346938775510204,
      "grad_norm": 1.4687588214874268,
      "learning_rate": 0.000192659793814433,
      "loss": 0.9778,
      "step": 229
    },
    {
      "epoch": 0.9387755102040817,
      "grad_norm": 2.190089225769043,
      "learning_rate": 0.00019261855670103093,
      "loss": 1.3526,
      "step": 230
    },
    {
      "epoch": 0.9428571428571428,
      "grad_norm": 1.568209171295166,
      "learning_rate": 0.00019257731958762889,
      "loss": 1.0192,
      "step": 231
    },
    {
      "epoch": 0.9469387755102041,
      "grad_norm": 1.7976233959197998,
      "learning_rate": 0.00019253608247422682,
      "loss": 1.1536,
      "step": 232
    },
    {
      "epoch": 0.9510204081632653,
      "grad_norm": 1.7449712753295898,
      "learning_rate": 0.00019249484536082475,
      "loss": 1.0672,
      "step": 233
    },
    {
      "epoch": 0.9551020408163265,
      "grad_norm": 1.4413225650787354,
      "learning_rate": 0.00019245360824742268,
      "loss": 1.1561,
      "step": 234
    },
    {
      "epoch": 0.9591836734693877,
      "grad_norm": 1.5554243326187134,
      "learning_rate": 0.0001924123711340206,
      "loss": 1.0305,
      "step": 235
    },
    {
      "epoch": 0.963265306122449,
      "grad_norm": 1.5797092914581299,
      "learning_rate": 0.00019237113402061857,
      "loss": 1.0073,
      "step": 236
    },
    {
      "epoch": 0.9673469387755103,
      "grad_norm": 1.7858996391296387,
      "learning_rate": 0.0001923298969072165,
      "loss": 1.1153,
      "step": 237
    },
    {
      "epoch": 0.9714285714285714,
      "grad_norm": 1.8335024118423462,
      "learning_rate": 0.00019228865979381446,
      "loss": 1.2021,
      "step": 238
    },
    {
      "epoch": 0.9755102040816327,
      "grad_norm": 1.7606735229492188,
      "learning_rate": 0.0001922474226804124,
      "loss": 0.9716,
      "step": 239
    },
    {
      "epoch": 0.9795918367346939,
      "grad_norm": 1.8341211080551147,
      "learning_rate": 0.00019220618556701032,
      "loss": 1.0994,
      "step": 240
    },
    {
      "epoch": 0.9836734693877551,
      "grad_norm": 1.7369986772537231,
      "learning_rate": 0.00019216494845360825,
      "loss": 0.9494,
      "step": 241
    },
    {
      "epoch": 0.9877551020408163,
      "grad_norm": 2.04683256149292,
      "learning_rate": 0.00019212371134020618,
      "loss": 1.1457,
      "step": 242
    },
    {
      "epoch": 0.9918367346938776,
      "grad_norm": 1.9625204801559448,
      "learning_rate": 0.00019208247422680414,
      "loss": 1.1138,
      "step": 243
    },
    {
      "epoch": 0.9959183673469387,
      "grad_norm": 1.805265188217163,
      "learning_rate": 0.00019204123711340207,
      "loss": 1.1378,
      "step": 244
    },
    {
      "epoch": 1.0,
      "grad_norm": 3.77846360206604,
      "learning_rate": 0.000192,
      "loss": 1.1897,
      "step": 245
    },
    {
      "epoch": 1.0040816326530613,
      "grad_norm": 1.7588015794754028,
      "learning_rate": 0.00019195876288659796,
      "loss": 0.9817,
      "step": 246
    },
    {
      "epoch": 1.0081632653061225,
      "grad_norm": 1.5311133861541748,
      "learning_rate": 0.0001919175257731959,
      "loss": 0.9259,
      "step": 247
    },
    {
      "epoch": 1.0122448979591836,
      "grad_norm": 1.448808193206787,
      "learning_rate": 0.00019187628865979383,
      "loss": 0.92,
      "step": 248
    },
    {
      "epoch": 1.0163265306122449,
      "grad_norm": 1.5817073583602905,
      "learning_rate": 0.00019183505154639176,
      "loss": 0.9449,
      "step": 249
    },
    {
      "epoch": 1.0204081632653061,
      "grad_norm": 1.7010719776153564,
      "learning_rate": 0.0001917938144329897,
      "loss": 1.0152,
      "step": 250
    },
    {
      "epoch": 1.0244897959183674,
      "grad_norm": 1.796923041343689,
      "learning_rate": 0.00019175257731958765,
      "loss": 0.9191,
      "step": 251
    },
    {
      "epoch": 1.0285714285714285,
      "grad_norm": 1.9228789806365967,
      "learning_rate": 0.00019171134020618558,
      "loss": 0.9612,
      "step": 252
    },
    {
      "epoch": 1.0326530612244897,
      "grad_norm": 2.125288724899292,
      "learning_rate": 0.0001916701030927835,
      "loss": 1.0701,
      "step": 253
    },
    {
      "epoch": 1.036734693877551,
      "grad_norm": 1.7312932014465332,
      "learning_rate": 0.00019162886597938147,
      "loss": 0.9676,
      "step": 254
    },
    {
      "epoch": 1.0408163265306123,
      "grad_norm": 2.1550769805908203,
      "learning_rate": 0.00019158762886597937,
      "loss": 1.1234,
      "step": 255
    },
    {
      "epoch": 1.0448979591836736,
      "grad_norm": 1.7792612314224243,
      "learning_rate": 0.00019154639175257733,
      "loss": 0.8194,
      "step": 256
    },
    {
      "epoch": 1.0489795918367346,
      "grad_norm": 1.8345670700073242,
      "learning_rate": 0.00019150515463917526,
      "loss": 1.0724,
      "step": 257
    },
    {
      "epoch": 1.0530612244897959,
      "grad_norm": 1.8539235591888428,
      "learning_rate": 0.0001914639175257732,
      "loss": 1.0445,
      "step": 258
    },
    {
      "epoch": 1.0571428571428572,
      "grad_norm": 1.9415907859802246,
      "learning_rate": 0.00019142268041237115,
      "loss": 1.1591,
      "step": 259
    },
    {
      "epoch": 1.0612244897959184,
      "grad_norm": 1.7553638219833374,
      "learning_rate": 0.00019138144329896908,
      "loss": 1.0273,
      "step": 260
    },
    {
      "epoch": 1.0653061224489795,
      "grad_norm": 1.694839358329773,
      "learning_rate": 0.00019134020618556704,
      "loss": 0.8865,
      "step": 261
    },
    {
      "epoch": 1.0693877551020408,
      "grad_norm": 1.7580968141555786,
      "learning_rate": 0.00019129896907216494,
      "loss": 0.8756,
      "step": 262
    },
    {
      "epoch": 1.073469387755102,
      "grad_norm": 2.2678728103637695,
      "learning_rate": 0.00019125773195876288,
      "loss": 1.1056,
      "step": 263
    },
    {
      "epoch": 1.0775510204081633,
      "grad_norm": 1.8339663743972778,
      "learning_rate": 0.00019121649484536083,
      "loss": 1.0534,
      "step": 264
    },
    {
      "epoch": 1.0816326530612246,
      "grad_norm": 1.7681471109390259,
      "learning_rate": 0.00019117525773195877,
      "loss": 0.9027,
      "step": 265
    },
    {
      "epoch": 1.0857142857142856,
      "grad_norm": 1.8999218940734863,
      "learning_rate": 0.00019113402061855672,
      "loss": 0.9438,
      "step": 266
    },
    {
      "epoch": 1.089795918367347,
      "grad_norm": 2.2726471424102783,
      "learning_rate": 0.00019109278350515466,
      "loss": 1.2459,
      "step": 267
    },
    {
      "epoch": 1.0938775510204082,
      "grad_norm": 2.342744827270508,
      "learning_rate": 0.0001910515463917526,
      "loss": 0.9737,
      "step": 268
    },
    {
      "epoch": 1.0979591836734695,
      "grad_norm": 1.8067961931228638,
      "learning_rate": 0.00019101030927835052,
      "loss": 0.9053,
      "step": 269
    },
    {
      "epoch": 1.1020408163265305,
      "grad_norm": 1.9644644260406494,
      "learning_rate": 0.00019096907216494845,
      "loss": 0.8861,
      "step": 270
    },
    {
      "epoch": 1.1061224489795918,
      "grad_norm": 1.7994033098220825,
      "learning_rate": 0.0001909278350515464,
      "loss": 0.9362,
      "step": 271
    },
    {
      "epoch": 1.110204081632653,
      "grad_norm": 2.011180877685547,
      "learning_rate": 0.00019088659793814434,
      "loss": 1.037,
      "step": 272
    },
    {
      "epoch": 1.1142857142857143,
      "grad_norm": 1.8873591423034668,
      "learning_rate": 0.00019084536082474227,
      "loss": 0.8788,
      "step": 273
    },
    {
      "epoch": 1.1183673469387756,
      "grad_norm": 1.828611969947815,
      "learning_rate": 0.00019080412371134023,
      "loss": 0.8474,
      "step": 274
    },
    {
      "epoch": 1.1224489795918366,
      "grad_norm": 2.0071780681610107,
      "learning_rate": 0.00019076288659793816,
      "loss": 0.9158,
      "step": 275
    },
    {
      "epoch": 1.126530612244898,
      "grad_norm": 2.0434958934783936,
      "learning_rate": 0.0001907216494845361,
      "loss": 0.9316,
      "step": 276
    },
    {
      "epoch": 1.1306122448979592,
      "grad_norm": 2.1216893196105957,
      "learning_rate": 0.00019068041237113402,
      "loss": 0.8922,
      "step": 277
    },
    {
      "epoch": 1.1346938775510205,
      "grad_norm": 1.996096134185791,
      "learning_rate": 0.00019063917525773195,
      "loss": 1.1124,
      "step": 278
    },
    {
      "epoch": 1.1387755102040815,
      "grad_norm": 1.9550039768218994,
      "learning_rate": 0.0001905979381443299,
      "loss": 1.0362,
      "step": 279
    },
    {
      "epoch": 1.1428571428571428,
      "grad_norm": 2.104497194290161,
      "learning_rate": 0.00019055670103092784,
      "loss": 0.8753,
      "step": 280
    },
    {
      "epoch": 1.146938775510204,
      "grad_norm": 2.06042218208313,
      "learning_rate": 0.00019051546391752577,
      "loss": 0.9652,
      "step": 281
    },
    {
      "epoch": 1.1510204081632653,
      "grad_norm": 1.5714836120605469,
      "learning_rate": 0.00019047422680412373,
      "loss": 0.8939,
      "step": 282
    },
    {
      "epoch": 1.1551020408163266,
      "grad_norm": 1.8592835664749146,
      "learning_rate": 0.00019043298969072166,
      "loss": 0.8912,
      "step": 283
    },
    {
      "epoch": 1.1591836734693877,
      "grad_norm": 1.8848159313201904,
      "learning_rate": 0.0001903917525773196,
      "loss": 0.9092,
      "step": 284
    },
    {
      "epoch": 1.163265306122449,
      "grad_norm": 1.587337851524353,
      "learning_rate": 0.00019035051546391753,
      "loss": 0.9231,
      "step": 285
    },
    {
      "epoch": 1.1673469387755102,
      "grad_norm": 2.01411509513855,
      "learning_rate": 0.00019030927835051546,
      "loss": 0.8499,
      "step": 286
    },
    {
      "epoch": 1.1714285714285715,
      "grad_norm": 1.9565562009811401,
      "learning_rate": 0.00019026804123711342,
      "loss": 0.9543,
      "step": 287
    },
    {
      "epoch": 1.1755102040816325,
      "grad_norm": 1.9016300439834595,
      "learning_rate": 0.00019022680412371135,
      "loss": 0.9238,
      "step": 288
    },
    {
      "epoch": 1.1795918367346938,
      "grad_norm": 2.049466371536255,
      "learning_rate": 0.0001901855670103093,
      "loss": 0.9149,
      "step": 289
    },
    {
      "epoch": 1.183673469387755,
      "grad_norm": 1.8300678730010986,
      "learning_rate": 0.00019014432989690724,
      "loss": 0.9623,
      "step": 290
    },
    {
      "epoch": 1.1877551020408164,
      "grad_norm": 1.761269211769104,
      "learning_rate": 0.00019010309278350514,
      "loss": 0.8124,
      "step": 291
    },
    {
      "epoch": 1.1918367346938776,
      "grad_norm": 1.817646861076355,
      "learning_rate": 0.0001900618556701031,
      "loss": 0.7615,
      "step": 292
    },
    {
      "epoch": 1.1959183673469387,
      "grad_norm": 2.321032762527466,
      "learning_rate": 0.00019002061855670103,
      "loss": 0.9618,
      "step": 293
    },
    {
      "epoch": 1.2,
      "grad_norm": 2.4044711589813232,
      "learning_rate": 0.000189979381443299,
      "loss": 1.0345,
      "step": 294
    },
    {
      "epoch": 1.2040816326530612,
      "grad_norm": 2.0030622482299805,
      "learning_rate": 0.00018993814432989692,
      "loss": 0.8985,
      "step": 295
    },
    {
      "epoch": 1.2081632653061225,
      "grad_norm": 1.99470853805542,
      "learning_rate": 0.00018989690721649485,
      "loss": 0.8336,
      "step": 296
    },
    {
      "epoch": 1.2122448979591836,
      "grad_norm": 1.914214015007019,
      "learning_rate": 0.0001898556701030928,
      "loss": 0.8849,
      "step": 297
    },
    {
      "epoch": 1.2163265306122448,
      "grad_norm": 1.9859157800674438,
      "learning_rate": 0.00018981443298969071,
      "loss": 0.949,
      "step": 298
    },
    {
      "epoch": 1.220408163265306,
      "grad_norm": 1.9759947061538696,
      "learning_rate": 0.00018977319587628867,
      "loss": 1.0184,
      "step": 299
    },
    {
      "epoch": 1.2244897959183674,
      "grad_norm": 1.8571339845657349,
      "learning_rate": 0.0001897319587628866,
      "loss": 0.9308,
      "step": 300
    },
    {
      "epoch": 1.2285714285714286,
      "grad_norm": 2.12715744972229,
      "learning_rate": 0.00018969072164948454,
      "loss": 1.0302,
      "step": 301
    },
    {
      "epoch": 1.2326530612244897,
      "grad_norm": 1.795554518699646,
      "learning_rate": 0.0001896494845360825,
      "loss": 0.8105,
      "step": 302
    },
    {
      "epoch": 1.236734693877551,
      "grad_norm": 2.2372424602508545,
      "learning_rate": 0.00018960824742268043,
      "loss": 0.9258,
      "step": 303
    },
    {
      "epoch": 1.2408163265306122,
      "grad_norm": 1.7019048929214478,
      "learning_rate": 0.00018956701030927836,
      "loss": 0.7503,
      "step": 304
    },
    {
      "epoch": 1.2448979591836735,
      "grad_norm": 1.889522910118103,
      "learning_rate": 0.0001895257731958763,
      "loss": 0.8877,
      "step": 305
    },
    {
      "epoch": 1.2489795918367346,
      "grad_norm": 2.112791061401367,
      "learning_rate": 0.00018948453608247422,
      "loss": 0.9838,
      "step": 306
    },
    {
      "epoch": 1.2530612244897958,
      "grad_norm": 2.0793678760528564,
      "learning_rate": 0.00018944329896907218,
      "loss": 0.8074,
      "step": 307
    },
    {
      "epoch": 1.2571428571428571,
      "grad_norm": 2.1511895656585693,
      "learning_rate": 0.0001894020618556701,
      "loss": 1.0013,
      "step": 308
    },
    {
      "epoch": 1.2612244897959184,
      "grad_norm": 1.9700109958648682,
      "learning_rate": 0.00018936082474226804,
      "loss": 0.9143,
      "step": 309
    },
    {
      "epoch": 1.2653061224489797,
      "grad_norm": 1.9267433881759644,
      "learning_rate": 0.000189319587628866,
      "loss": 0.9592,
      "step": 310
    },
    {
      "epoch": 1.269387755102041,
      "grad_norm": 1.9124420881271362,
      "learning_rate": 0.00018927835051546393,
      "loss": 0.9206,
      "step": 311
    },
    {
      "epoch": 1.273469387755102,
      "grad_norm": 2.074432373046875,
      "learning_rate": 0.00018923711340206186,
      "loss": 0.9283,
      "step": 312
    },
    {
      "epoch": 1.2775510204081633,
      "grad_norm": 2.376875162124634,
      "learning_rate": 0.0001891958762886598,
      "loss": 0.8126,
      "step": 313
    },
    {
      "epoch": 1.2816326530612245,
      "grad_norm": 1.8736811876296997,
      "learning_rate": 0.00018915463917525772,
      "loss": 0.8826,
      "step": 314
    },
    {
      "epoch": 1.2857142857142856,
      "grad_norm": 1.8584867715835571,
      "learning_rate": 0.00018911340206185568,
      "loss": 0.8425,
      "step": 315
    },
    {
      "epoch": 1.2897959183673469,
      "grad_norm": 2.025859832763672,
      "learning_rate": 0.0001890721649484536,
      "loss": 0.9891,
      "step": 316
    },
    {
      "epoch": 1.2938775510204081,
      "grad_norm": 2.0063204765319824,
      "learning_rate": 0.00018903092783505157,
      "loss": 0.9917,
      "step": 317
    },
    {
      "epoch": 1.2979591836734694,
      "grad_norm": 1.835485577583313,
      "learning_rate": 0.0001889896907216495,
      "loss": 0.7839,
      "step": 318
    },
    {
      "epoch": 1.3020408163265307,
      "grad_norm": 2.1408565044403076,
      "learning_rate": 0.00018894845360824743,
      "loss": 0.9622,
      "step": 319
    },
    {
      "epoch": 1.306122448979592,
      "grad_norm": 1.9640854597091675,
      "learning_rate": 0.00018890721649484537,
      "loss": 1.0356,
      "step": 320
    },
    {
      "epoch": 1.310204081632653,
      "grad_norm": 2.0710761547088623,
      "learning_rate": 0.0001888659793814433,
      "loss": 0.8705,
      "step": 321
    },
    {
      "epoch": 1.3142857142857143,
      "grad_norm": 1.6879297494888306,
      "learning_rate": 0.00018882474226804126,
      "loss": 0.8222,
      "step": 322
    },
    {
      "epoch": 1.3183673469387756,
      "grad_norm": 2.19336199760437,
      "learning_rate": 0.00018878350515463919,
      "loss": 0.926,
      "step": 323
    },
    {
      "epoch": 1.3224489795918366,
      "grad_norm": 2.207548141479492,
      "learning_rate": 0.00018874226804123712,
      "loss": 0.9454,
      "step": 324
    },
    {
      "epoch": 1.3265306122448979,
      "grad_norm": 2.191011428833008,
      "learning_rate": 0.00018870103092783508,
      "loss": 0.9276,
      "step": 325
    },
    {
      "epoch": 1.3306122448979592,
      "grad_norm": 1.9423199892044067,
      "learning_rate": 0.000188659793814433,
      "loss": 0.8776,
      "step": 326
    },
    {
      "epoch": 1.3346938775510204,
      "grad_norm": 1.799981713294983,
      "learning_rate": 0.00018861855670103094,
      "loss": 0.7934,
      "step": 327
    },
    {
      "epoch": 1.3387755102040817,
      "grad_norm": 2.623899221420288,
      "learning_rate": 0.00018857731958762887,
      "loss": 1.042,
      "step": 328
    },
    {
      "epoch": 1.342857142857143,
      "grad_norm": 1.7059510946273804,
      "learning_rate": 0.0001885360824742268,
      "loss": 0.8308,
      "step": 329
    },
    {
      "epoch": 1.346938775510204,
      "grad_norm": 1.888381838798523,
      "learning_rate": 0.00018849484536082476,
      "loss": 1.0065,
      "step": 330
    },
    {
      "epoch": 1.3510204081632653,
      "grad_norm": 2.728586435317993,
      "learning_rate": 0.0001884536082474227,
      "loss": 0.7724,
      "step": 331
    },
    {
      "epoch": 1.3551020408163266,
      "grad_norm": 1.7725870609283447,
      "learning_rate": 0.00018841237113402065,
      "loss": 0.897,
      "step": 332
    },
    {
      "epoch": 1.3591836734693876,
      "grad_norm": 2.2030041217803955,
      "learning_rate": 0.00018837113402061858,
      "loss": 0.9073,
      "step": 333
    },
    {
      "epoch": 1.363265306122449,
      "grad_norm": 1.771498203277588,
      "learning_rate": 0.00018832989690721648,
      "loss": 0.8923,
      "step": 334
    },
    {
      "epoch": 1.3673469387755102,
      "grad_norm": 1.7947092056274414,
      "learning_rate": 0.00018828865979381444,
      "loss": 0.8544,
      "step": 335
    },
    {
      "epoch": 1.3714285714285714,
      "grad_norm": 1.7715345621109009,
      "learning_rate": 0.00018824742268041237,
      "loss": 0.814,
      "step": 336
    },
    {
      "epoch": 1.3755102040816327,
      "grad_norm": 1.9237682819366455,
      "learning_rate": 0.00018820618556701033,
      "loss": 0.855,
      "step": 337
    },
    {
      "epoch": 1.379591836734694,
      "grad_norm": 1.7575249671936035,
      "learning_rate": 0.00018816494845360826,
      "loss": 0.837,
      "step": 338
    },
    {
      "epoch": 1.383673469387755,
      "grad_norm": 1.6998621225357056,
      "learning_rate": 0.0001881237113402062,
      "loss": 0.779,
      "step": 339
    },
    {
      "epoch": 1.3877551020408163,
      "grad_norm": 1.7740877866744995,
      "learning_rate": 0.00018808247422680413,
      "loss": 0.7911,
      "step": 340
    },
    {
      "epoch": 1.3918367346938776,
      "grad_norm": 2.5324671268463135,
      "learning_rate": 0.00018804123711340206,
      "loss": 1.0246,
      "step": 341
    },
    {
      "epoch": 1.3959183673469386,
      "grad_norm": 2.0419769287109375,
      "learning_rate": 0.000188,
      "loss": 1.0582,
      "step": 342
    },
    {
      "epoch": 1.4,
      "grad_norm": 1.911015272140503,
      "learning_rate": 0.00018795876288659795,
      "loss": 0.8313,
      "step": 343
    },
    {
      "epoch": 1.4040816326530612,
      "grad_norm": 2.2890918254852295,
      "learning_rate": 0.00018791752577319588,
      "loss": 1.0087,
      "step": 344
    },
    {
      "epoch": 1.4081632653061225,
      "grad_norm": 2.0341975688934326,
      "learning_rate": 0.00018787628865979384,
      "loss": 0.8561,
      "step": 345
    },
    {
      "epoch": 1.4122448979591837,
      "grad_norm": 1.87313973903656,
      "learning_rate": 0.00018783505154639177,
      "loss": 0.8676,
      "step": 346
    },
    {
      "epoch": 1.416326530612245,
      "grad_norm": 1.9303194284439087,
      "learning_rate": 0.0001877938144329897,
      "loss": 0.8861,
      "step": 347
    },
    {
      "epoch": 1.420408163265306,
      "grad_norm": 1.7784090042114258,
      "learning_rate": 0.00018775257731958763,
      "loss": 0.9408,
      "step": 348
    },
    {
      "epoch": 1.4244897959183673,
      "grad_norm": 2.0797905921936035,
      "learning_rate": 0.00018771134020618556,
      "loss": 0.9952,
      "step": 349
    },
    {
      "epoch": 1.4285714285714286,
      "grad_norm": 2.115452766418457,
      "learning_rate": 0.00018767010309278352,
      "loss": 0.995,
      "step": 350
    },
    {
      "epoch": 1.4326530612244899,
      "grad_norm": 1.817739486694336,
      "learning_rate": 0.00018762886597938145,
      "loss": 0.8129,
      "step": 351
    },
    {
      "epoch": 1.436734693877551,
      "grad_norm": 1.9469597339630127,
      "learning_rate": 0.00018758762886597938,
      "loss": 0.9899,
      "step": 352
    },
    {
      "epoch": 1.4408163265306122,
      "grad_norm": 1.7305095195770264,
      "learning_rate": 0.00018754639175257734,
      "loss": 0.7805,
      "step": 353
    },
    {
      "epoch": 1.4448979591836735,
      "grad_norm": 1.753750205039978,
      "learning_rate": 0.00018750515463917527,
      "loss": 0.9121,
      "step": 354
    },
    {
      "epoch": 1.4489795918367347,
      "grad_norm": 2.1617753505706787,
      "learning_rate": 0.0001874639175257732,
      "loss": 0.9,
      "step": 355
    },
    {
      "epoch": 1.453061224489796,
      "grad_norm": 1.6653635501861572,
      "learning_rate": 0.00018742268041237114,
      "loss": 0.7629,
      "step": 356
    },
    {
      "epoch": 1.457142857142857,
      "grad_norm": 1.6548264026641846,
      "learning_rate": 0.00018738144329896907,
      "loss": 0.7487,
      "step": 357
    },
    {
      "epoch": 1.4612244897959183,
      "grad_norm": 2.0258255004882812,
      "learning_rate": 0.00018734020618556702,
      "loss": 0.8701,
      "step": 358
    },
    {
      "epoch": 1.4653061224489796,
      "grad_norm": 2.1219940185546875,
      "learning_rate": 0.00018729896907216496,
      "loss": 0.8476,
      "step": 359
    },
    {
      "epoch": 1.469387755102041,
      "grad_norm": 2.194650650024414,
      "learning_rate": 0.00018725773195876291,
      "loss": 0.8176,
      "step": 360
    },
    {
      "epoch": 1.473469387755102,
      "grad_norm": 1.8167599439620972,
      "learning_rate": 0.00018721649484536085,
      "loss": 0.7969,
      "step": 361
    },
    {
      "epoch": 1.4775510204081632,
      "grad_norm": 2.1128060817718506,
      "learning_rate": 0.00018717525773195878,
      "loss": 0.7971,
      "step": 362
    },
    {
      "epoch": 1.4816326530612245,
      "grad_norm": 2.428452253341675,
      "learning_rate": 0.0001871340206185567,
      "loss": 1.0814,
      "step": 363
    },
    {
      "epoch": 1.4857142857142858,
      "grad_norm": 1.7615209817886353,
      "learning_rate": 0.00018709278350515464,
      "loss": 0.7445,
      "step": 364
    },
    {
      "epoch": 1.489795918367347,
      "grad_norm": 1.981575608253479,
      "learning_rate": 0.0001870515463917526,
      "loss": 0.7981,
      "step": 365
    },
    {
      "epoch": 1.493877551020408,
      "grad_norm": 1.9907821416854858,
      "learning_rate": 0.00018701030927835053,
      "loss": 0.8811,
      "step": 366
    },
    {
      "epoch": 1.4979591836734694,
      "grad_norm": 2.147002696990967,
      "learning_rate": 0.00018696907216494846,
      "loss": 0.8502,
      "step": 367
    },
    {
      "epoch": 1.5020408163265306,
      "grad_norm": 2.2369649410247803,
      "learning_rate": 0.00018692783505154642,
      "loss": 0.9621,
      "step": 368
    },
    {
      "epoch": 1.5061224489795917,
      "grad_norm": 2.8229403495788574,
      "learning_rate": 0.00018688659793814432,
      "loss": 0.8247,
      "step": 369
    },
    {
      "epoch": 1.510204081632653,
      "grad_norm": 2.1590771675109863,
      "learning_rate": 0.00018684536082474225,
      "loss": 0.9298,
      "step": 370
    },
    {
      "epoch": 1.5142857142857142,
      "grad_norm": 1.8673698902130127,
      "learning_rate": 0.0001868041237113402,
      "loss": 0.7792,
      "step": 371
    },
    {
      "epoch": 1.5183673469387755,
      "grad_norm": 2.3442413806915283,
      "learning_rate": 0.00018676288659793814,
      "loss": 1.0837,
      "step": 372
    },
    {
      "epoch": 1.5224489795918368,
      "grad_norm": 2.1104352474212646,
      "learning_rate": 0.0001867216494845361,
      "loss": 0.8322,
      "step": 373
    },
    {
      "epoch": 1.526530612244898,
      "grad_norm": 1.75258207321167,
      "learning_rate": 0.00018668041237113403,
      "loss": 0.7758,
      "step": 374
    },
    {
      "epoch": 1.5306122448979593,
      "grad_norm": 1.5721025466918945,
      "learning_rate": 0.00018663917525773196,
      "loss": 0.7652,
      "step": 375
    },
    {
      "epoch": 1.5346938775510204,
      "grad_norm": 2.0825796127319336,
      "learning_rate": 0.0001865979381443299,
      "loss": 0.8131,
      "step": 376
    },
    {
      "epoch": 1.5387755102040817,
      "grad_norm": 2.827152967453003,
      "learning_rate": 0.00018655670103092783,
      "loss": 0.9861,
      "step": 377
    },
    {
      "epoch": 1.5428571428571427,
      "grad_norm": 2.323899030685425,
      "learning_rate": 0.00018651546391752579,
      "loss": 0.9001,
      "step": 378
    },
    {
      "epoch": 1.546938775510204,
      "grad_norm": 1.8089139461517334,
      "learning_rate": 0.00018647422680412372,
      "loss": 0.7888,
      "step": 379
    },
    {
      "epoch": 1.5510204081632653,
      "grad_norm": 1.8917745351791382,
      "learning_rate": 0.00018643298969072165,
      "loss": 0.8131,
      "step": 380
    },
    {
      "epoch": 1.5551020408163265,
      "grad_norm": 2.289212703704834,
      "learning_rate": 0.0001863917525773196,
      "loss": 0.8996,
      "step": 381
    },
    {
      "epoch": 1.5591836734693878,
      "grad_norm": 2.2638027667999268,
      "learning_rate": 0.00018635051546391754,
      "loss": 0.9367,
      "step": 382
    },
    {
      "epoch": 1.563265306122449,
      "grad_norm": 2.0261545181274414,
      "learning_rate": 0.00018630927835051547,
      "loss": 0.8955,
      "step": 383
    },
    {
      "epoch": 1.5673469387755103,
      "grad_norm": 2.0411875247955322,
      "learning_rate": 0.0001862680412371134,
      "loss": 0.8317,
      "step": 384
    },
    {
      "epoch": 1.5714285714285714,
      "grad_norm": 2.013906955718994,
      "learning_rate": 0.00018622680412371133,
      "loss": 0.8028,
      "step": 385
    },
    {
      "epoch": 1.5755102040816327,
      "grad_norm": 2.0231902599334717,
      "learning_rate": 0.0001861855670103093,
      "loss": 0.8548,
      "step": 386
    },
    {
      "epoch": 1.5795918367346937,
      "grad_norm": 1.8851096630096436,
      "learning_rate": 0.00018614432989690722,
      "loss": 0.847,
      "step": 387
    },
    {
      "epoch": 1.583673469387755,
      "grad_norm": 2.169675350189209,
      "learning_rate": 0.00018610309278350518,
      "loss": 0.7986,
      "step": 388
    },
    {
      "epoch": 1.5877551020408163,
      "grad_norm": 2.1230080127716064,
      "learning_rate": 0.0001860618556701031,
      "loss": 0.7996,
      "step": 389
    },
    {
      "epoch": 1.5918367346938775,
      "grad_norm": 2.1553924083709717,
      "learning_rate": 0.00018602061855670104,
      "loss": 0.8498,
      "step": 390
    },
    {
      "epoch": 1.5959183673469388,
      "grad_norm": 1.7697370052337646,
      "learning_rate": 0.00018597938144329897,
      "loss": 0.7233,
      "step": 391
    },
    {
      "epoch": 1.6,
      "grad_norm": 2.313502788543701,
      "learning_rate": 0.0001859381443298969,
      "loss": 0.8191,
      "step": 392
    },
    {
      "epoch": 1.6040816326530614,
      "grad_norm": 2.1246285438537598,
      "learning_rate": 0.00018589690721649486,
      "loss": 0.8228,
      "step": 393
    },
    {
      "epoch": 1.6081632653061224,
      "grad_norm": 2.4803054332733154,
      "learning_rate": 0.0001858556701030928,
      "loss": 0.7983,
      "step": 394
    },
    {
      "epoch": 1.6122448979591837,
      "grad_norm": 2.3022589683532715,
      "learning_rate": 0.00018581443298969073,
      "loss": 0.9318,
      "step": 395
    },
    {
      "epoch": 1.6163265306122447,
      "grad_norm": 3.0277132987976074,
      "learning_rate": 0.00018577319587628868,
      "loss": 0.9927,
      "step": 396
    },
    {
      "epoch": 1.620408163265306,
      "grad_norm": 2.1701667308807373,
      "learning_rate": 0.00018573195876288662,
      "loss": 0.6491,
      "step": 397
    },
    {
      "epoch": 1.6244897959183673,
      "grad_norm": 2.2437407970428467,
      "learning_rate": 0.00018569072164948455,
      "loss": 0.818,
      "step": 398
    },
    {
      "epoch": 1.6285714285714286,
      "grad_norm": 2.1952779293060303,
      "learning_rate": 0.00018564948453608248,
      "loss": 0.8447,
      "step": 399
    },
    {
      "epoch": 1.6326530612244898,
      "grad_norm": 2.0542643070220947,
      "learning_rate": 0.0001856082474226804,
      "loss": 0.8579,
      "step": 400
    },
    {
      "epoch": 1.636734693877551,
      "grad_norm": 1.869281530380249,
      "learning_rate": 0.00018556701030927837,
      "loss": 0.7942,
      "step": 401
    },
    {
      "epoch": 1.6408163265306124,
      "grad_norm": 2.6009740829467773,
      "learning_rate": 0.0001855257731958763,
      "loss": 0.8733,
      "step": 402
    },
    {
      "epoch": 1.6448979591836734,
      "grad_norm": 1.9820573329925537,
      "learning_rate": 0.00018548453608247423,
      "loss": 0.7618,
      "step": 403
    },
    {
      "epoch": 1.6489795918367347,
      "grad_norm": 2.000697612762451,
      "learning_rate": 0.0001854432989690722,
      "loss": 0.9015,
      "step": 404
    },
    {
      "epoch": 1.6530612244897958,
      "grad_norm": 2.234708070755005,
      "learning_rate": 0.0001854020618556701,
      "loss": 0.9789,
      "step": 405
    },
    {
      "epoch": 1.657142857142857,
      "grad_norm": 2.1884284019470215,
      "learning_rate": 0.00018536082474226805,
      "loss": 0.9614,
      "step": 406
    },
    {
      "epoch": 1.6612244897959183,
      "grad_norm": 1.5552462339401245,
      "learning_rate": 0.00018531958762886598,
      "loss": 0.6902,
      "step": 407
    },
    {
      "epoch": 1.6653061224489796,
      "grad_norm": 1.9280825853347778,
      "learning_rate": 0.00018527835051546391,
      "loss": 0.9439,
      "step": 408
    },
    {
      "epoch": 1.6693877551020408,
      "grad_norm": 1.8427311182022095,
      "learning_rate": 0.00018523711340206187,
      "loss": 0.71,
      "step": 409
    },
    {
      "epoch": 1.6734693877551021,
      "grad_norm": 1.9870539903640747,
      "learning_rate": 0.0001851958762886598,
      "loss": 0.7822,
      "step": 410
    },
    {
      "epoch": 1.6775510204081634,
      "grad_norm": 2.170661687850952,
      "learning_rate": 0.00018515463917525776,
      "loss": 0.843,
      "step": 411
    },
    {
      "epoch": 1.6816326530612244,
      "grad_norm": 2.0425848960876465,
      "learning_rate": 0.00018511340206185567,
      "loss": 0.6849,
      "step": 412
    },
    {
      "epoch": 1.6857142857142857,
      "grad_norm": 2.1786081790924072,
      "learning_rate": 0.0001850721649484536,
      "loss": 0.8924,
      "step": 413
    },
    {
      "epoch": 1.689795918367347,
      "grad_norm": 2.096743106842041,
      "learning_rate": 0.00018503092783505156,
      "loss": 0.8553,
      "step": 414
    },
    {
      "epoch": 1.693877551020408,
      "grad_norm": 2.6579160690307617,
      "learning_rate": 0.0001849896907216495,
      "loss": 1.0342,
      "step": 415
    },
    {
      "epoch": 1.6979591836734693,
      "grad_norm": 2.0985357761383057,
      "learning_rate": 0.00018494845360824745,
      "loss": 0.6521,
      "step": 416
    },
    {
      "epoch": 1.7020408163265306,
      "grad_norm": 1.8569059371948242,
      "learning_rate": 0.00018490721649484538,
      "loss": 0.7503,
      "step": 417
    },
    {
      "epoch": 1.7061224489795919,
      "grad_norm": 2.4250965118408203,
      "learning_rate": 0.0001848659793814433,
      "loss": 0.8118,
      "step": 418
    },
    {
      "epoch": 1.7102040816326531,
      "grad_norm": 2.287116050720215,
      "learning_rate": 0.00018482474226804124,
      "loss": 0.9122,
      "step": 419
    },
    {
      "epoch": 1.7142857142857144,
      "grad_norm": 1.7201660871505737,
      "learning_rate": 0.00018478350515463917,
      "loss": 0.8617,
      "step": 420
    },
    {
      "epoch": 1.7183673469387755,
      "grad_norm": 1.8569424152374268,
      "learning_rate": 0.00018474226804123713,
      "loss": 0.8556,
      "step": 421
    },
    {
      "epoch": 1.7224489795918367,
      "grad_norm": 1.5368573665618896,
      "learning_rate": 0.00018470103092783506,
      "loss": 0.6981,
      "step": 422
    },
    {
      "epoch": 1.726530612244898,
      "grad_norm": 2.1237990856170654,
      "learning_rate": 0.000184659793814433,
      "loss": 0.8435,
      "step": 423
    },
    {
      "epoch": 1.730612244897959,
      "grad_norm": 2.140834093093872,
      "learning_rate": 0.00018461855670103095,
      "loss": 0.7721,
      "step": 424
    },
    {
      "epoch": 1.7346938775510203,
      "grad_norm": 2.314633846282959,
      "learning_rate": 0.00018457731958762888,
      "loss": 0.9536,
      "step": 425
    },
    {
      "epoch": 1.7387755102040816,
      "grad_norm": 2.201343536376953,
      "learning_rate": 0.0001845360824742268,
      "loss": 0.8433,
      "step": 426
    },
    {
      "epoch": 1.7428571428571429,
      "grad_norm": 2.109233856201172,
      "learning_rate": 0.00018449484536082474,
      "loss": 0.7261,
      "step": 427
    },
    {
      "epoch": 1.7469387755102042,
      "grad_norm": 2.0495686531066895,
      "learning_rate": 0.00018445360824742267,
      "loss": 0.792,
      "step": 428
    },
    {
      "epoch": 1.7510204081632654,
      "grad_norm": 2.100611925125122,
      "learning_rate": 0.00018441237113402063,
      "loss": 0.757,
      "step": 429
    },
    {
      "epoch": 1.7551020408163265,
      "grad_norm": 2.2383856773376465,
      "learning_rate": 0.00018437113402061856,
      "loss": 0.825,
      "step": 430
    },
    {
      "epoch": 1.7591836734693878,
      "grad_norm": 2.141606330871582,
      "learning_rate": 0.0001843298969072165,
      "loss": 0.8004,
      "step": 431
    },
    {
      "epoch": 1.763265306122449,
      "grad_norm": 2.1188409328460693,
      "learning_rate": 0.00018428865979381445,
      "loss": 0.8391,
      "step": 432
    },
    {
      "epoch": 1.76734693877551,
      "grad_norm": 2.1668598651885986,
      "learning_rate": 0.00018424742268041239,
      "loss": 0.9095,
      "step": 433
    },
    {
      "epoch": 1.7714285714285714,
      "grad_norm": 1.9107953310012817,
      "learning_rate": 0.00018420618556701032,
      "loss": 0.8797,
      "step": 434
    },
    {
      "epoch": 1.7755102040816326,
      "grad_norm": 2.441955089569092,
      "learning_rate": 0.00018416494845360825,
      "loss": 1.0047,
      "step": 435
    },
    {
      "epoch": 1.779591836734694,
      "grad_norm": 2.256410598754883,
      "learning_rate": 0.00018412371134020618,
      "loss": 0.759,
      "step": 436
    },
    {
      "epoch": 1.7836734693877552,
      "grad_norm": 2.2383105754852295,
      "learning_rate": 0.00018408247422680414,
      "loss": 0.8989,
      "step": 437
    },
    {
      "epoch": 1.7877551020408164,
      "grad_norm": 1.9766888618469238,
      "learning_rate": 0.00018404123711340207,
      "loss": 0.7845,
      "step": 438
    },
    {
      "epoch": 1.7918367346938775,
      "grad_norm": 2.5338211059570312,
      "learning_rate": 0.00018400000000000003,
      "loss": 0.8817,
      "step": 439
    },
    {
      "epoch": 1.7959183673469388,
      "grad_norm": 2.3576629161834717,
      "learning_rate": 0.00018395876288659796,
      "loss": 0.8735,
      "step": 440
    },
    {
      "epoch": 1.8,
      "grad_norm": 2.353837251663208,
      "learning_rate": 0.00018391752577319586,
      "loss": 0.8042,
      "step": 441
    },
    {
      "epoch": 1.804081632653061,
      "grad_norm": 1.8047913312911987,
      "learning_rate": 0.00018387628865979382,
      "loss": 0.7368,
      "step": 442
    },
    {
      "epoch": 1.8081632653061224,
      "grad_norm": 2.1651501655578613,
      "learning_rate": 0.00018383505154639175,
      "loss": 0.8718,
      "step": 443
    },
    {
      "epoch": 1.8122448979591836,
      "grad_norm": 2.033853769302368,
      "learning_rate": 0.0001837938144329897,
      "loss": 0.7591,
      "step": 444
    },
    {
      "epoch": 1.816326530612245,
      "grad_norm": 2.1229255199432373,
      "learning_rate": 0.00018375257731958764,
      "loss": 0.8355,
      "step": 445
    },
    {
      "epoch": 1.8204081632653062,
      "grad_norm": 1.83967125415802,
      "learning_rate": 0.00018371134020618557,
      "loss": 0.8108,
      "step": 446
    },
    {
      "epoch": 1.8244897959183675,
      "grad_norm": 2.265578508377075,
      "learning_rate": 0.00018367010309278353,
      "loss": 0.8282,
      "step": 447
    },
    {
      "epoch": 1.8285714285714287,
      "grad_norm": 2.186687707901001,
      "learning_rate": 0.00018362886597938144,
      "loss": 0.9035,
      "step": 448
    },
    {
      "epoch": 1.8326530612244898,
      "grad_norm": 1.99631929397583,
      "learning_rate": 0.0001835876288659794,
      "loss": 0.6709,
      "step": 449
    },
    {
      "epoch": 1.836734693877551,
      "grad_norm": 2.337547540664673,
      "learning_rate": 0.00018354639175257733,
      "loss": 0.7645,
      "step": 450
    },
    {
      "epoch": 1.8408163265306121,
      "grad_norm": 2.3881113529205322,
      "learning_rate": 0.00018350515463917526,
      "loss": 0.7198,
      "step": 451
    },
    {
      "epoch": 1.8448979591836734,
      "grad_norm": 2.4843719005584717,
      "learning_rate": 0.00018346391752577322,
      "loss": 0.8379,
      "step": 452
    },
    {
      "epoch": 1.8489795918367347,
      "grad_norm": 2.2955751419067383,
      "learning_rate": 0.00018342268041237115,
      "loss": 0.9046,
      "step": 453
    },
    {
      "epoch": 1.853061224489796,
      "grad_norm": 2.157318353652954,
      "learning_rate": 0.00018338144329896908,
      "loss": 0.7638,
      "step": 454
    },
    {
      "epoch": 1.8571428571428572,
      "grad_norm": 1.7531176805496216,
      "learning_rate": 0.000183340206185567,
      "loss": 0.6163,
      "step": 455
    },
    {
      "epoch": 1.8612244897959185,
      "grad_norm": 1.982662558555603,
      "learning_rate": 0.00018329896907216494,
      "loss": 0.7757,
      "step": 456
    },
    {
      "epoch": 1.8653061224489798,
      "grad_norm": 2.121814250946045,
      "learning_rate": 0.0001832577319587629,
      "loss": 0.7735,
      "step": 457
    },
    {
      "epoch": 1.8693877551020408,
      "grad_norm": 1.8528540134429932,
      "learning_rate": 0.00018321649484536083,
      "loss": 0.8445,
      "step": 458
    },
    {
      "epoch": 1.873469387755102,
      "grad_norm": 2.0248777866363525,
      "learning_rate": 0.0001831752577319588,
      "loss": 0.7231,
      "step": 459
    },
    {
      "epoch": 1.8775510204081631,
      "grad_norm": 2.207071542739868,
      "learning_rate": 0.00018313402061855672,
      "loss": 0.886,
      "step": 460
    },
    {
      "epoch": 1.8816326530612244,
      "grad_norm": 2.2363946437835693,
      "learning_rate": 0.00018309278350515465,
      "loss": 0.9159,
      "step": 461
    },
    {
      "epoch": 1.8857142857142857,
      "grad_norm": 2.0062365531921387,
      "learning_rate": 0.00018305154639175258,
      "loss": 0.8085,
      "step": 462
    },
    {
      "epoch": 1.889795918367347,
      "grad_norm": 1.8042330741882324,
      "learning_rate": 0.0001830103092783505,
      "loss": 0.7313,
      "step": 463
    },
    {
      "epoch": 1.8938775510204082,
      "grad_norm": 1.9270108938217163,
      "learning_rate": 0.00018296907216494844,
      "loss": 0.7655,
      "step": 464
    },
    {
      "epoch": 1.8979591836734695,
      "grad_norm": 2.187624931335449,
      "learning_rate": 0.0001829278350515464,
      "loss": 0.7687,
      "step": 465
    },
    {
      "epoch": 1.9020408163265308,
      "grad_norm": 2.1085598468780518,
      "learning_rate": 0.00018288659793814433,
      "loss": 0.7909,
      "step": 466
    },
    {
      "epoch": 1.9061224489795918,
      "grad_norm": 2.0236117839813232,
      "learning_rate": 0.0001828453608247423,
      "loss": 0.8214,
      "step": 467
    },
    {
      "epoch": 1.910204081632653,
      "grad_norm": 2.2604100704193115,
      "learning_rate": 0.00018280412371134022,
      "loss": 0.8137,
      "step": 468
    },
    {
      "epoch": 1.9142857142857141,
      "grad_norm": 2.04943585395813,
      "learning_rate": 0.00018276288659793816,
      "loss": 0.7536,
      "step": 469
    },
    {
      "epoch": 1.9183673469387754,
      "grad_norm": 2.548201084136963,
      "learning_rate": 0.0001827216494845361,
      "loss": 0.8578,
      "step": 470
    },
    {
      "epoch": 1.9224489795918367,
      "grad_norm": 1.9711936712265015,
      "learning_rate": 0.00018268041237113402,
      "loss": 0.7486,
      "step": 471
    },
    {
      "epoch": 1.926530612244898,
      "grad_norm": 2.2511017322540283,
      "learning_rate": 0.00018263917525773198,
      "loss": 0.8716,
      "step": 472
    },
    {
      "epoch": 1.9306122448979592,
      "grad_norm": 2.193706512451172,
      "learning_rate": 0.0001825979381443299,
      "loss": 0.834,
      "step": 473
    },
    {
      "epoch": 1.9346938775510205,
      "grad_norm": 2.399806499481201,
      "learning_rate": 0.00018255670103092784,
      "loss": 0.8939,
      "step": 474
    },
    {
      "epoch": 1.9387755102040818,
      "grad_norm": 1.905759334564209,
      "learning_rate": 0.0001825154639175258,
      "loss": 0.7665,
      "step": 475
    },
    {
      "epoch": 1.9428571428571428,
      "grad_norm": 1.9470696449279785,
      "learning_rate": 0.00018247422680412373,
      "loss": 0.7215,
      "step": 476
    },
    {
      "epoch": 1.9469387755102041,
      "grad_norm": 1.758159875869751,
      "learning_rate": 0.00018243298969072166,
      "loss": 0.7497,
      "step": 477
    },
    {
      "epoch": 1.9510204081632652,
      "grad_norm": 2.0728044509887695,
      "learning_rate": 0.0001823917525773196,
      "loss": 0.9222,
      "step": 478
    },
    {
      "epoch": 1.9551020408163264,
      "grad_norm": 1.9227901697158813,
      "learning_rate": 0.00018235051546391752,
      "loss": 0.6826,
      "step": 479
    },
    {
      "epoch": 1.9591836734693877,
      "grad_norm": 1.823971152305603,
      "learning_rate": 0.00018230927835051548,
      "loss": 0.7064,
      "step": 480
    },
    {
      "epoch": 1.963265306122449,
      "grad_norm": 1.878631830215454,
      "learning_rate": 0.0001822680412371134,
      "loss": 0.7425,
      "step": 481
    },
    {
      "epoch": 1.9673469387755103,
      "grad_norm": 1.951228380203247,
      "learning_rate": 0.00018222680412371137,
      "loss": 0.6603,
      "step": 482
    },
    {
      "epoch": 1.9714285714285715,
      "grad_norm": 1.9253891706466675,
      "learning_rate": 0.0001821855670103093,
      "loss": 0.6696,
      "step": 483
    },
    {
      "epoch": 1.9755102040816328,
      "grad_norm": 1.8028367757797241,
      "learning_rate": 0.0001821443298969072,
      "loss": 0.6681,
      "step": 484
    },
    {
      "epoch": 1.9795918367346939,
      "grad_norm": 2.4685006141662598,
      "learning_rate": 0.00018210309278350516,
      "loss": 0.9377,
      "step": 485
    },
    {
      "epoch": 1.9836734693877551,
      "grad_norm": 2.0567314624786377,
      "learning_rate": 0.0001820618556701031,
      "loss": 0.7707,
      "step": 486
    },
    {
      "epoch": 1.9877551020408162,
      "grad_norm": 2.025660276412964,
      "learning_rate": 0.00018202061855670105,
      "loss": 0.8241,
      "step": 487
    },
    {
      "epoch": 1.9918367346938775,
      "grad_norm": 2.2787885665893555,
      "learning_rate": 0.00018197938144329898,
      "loss": 0.7292,
      "step": 488
    },
    {
      "epoch": 1.9959183673469387,
      "grad_norm": 1.8271403312683105,
      "learning_rate": 0.00018193814432989692,
      "loss": 0.7025,
      "step": 489
    },
    {
      "epoch": 2.0,
      "grad_norm": 3.41102933883667,
      "learning_rate": 0.00018189690721649485,
      "loss": 0.909,
      "step": 490
    },
    {
      "epoch": 2.0040816326530613,
      "grad_norm": 1.9589842557907104,
      "learning_rate": 0.00018185567010309278,
      "loss": 0.8378,
      "step": 491
    },
    {
      "epoch": 2.0081632653061225,
      "grad_norm": 1.9336588382720947,
      "learning_rate": 0.0001818144329896907,
      "loss": 0.6764,
      "step": 492
    },
    {
      "epoch": 2.012244897959184,
      "grad_norm": 2.039583206176758,
      "learning_rate": 0.00018177319587628867,
      "loss": 0.6582,
      "step": 493
    },
    {
      "epoch": 2.016326530612245,
      "grad_norm": 2.267021417617798,
      "learning_rate": 0.0001817319587628866,
      "loss": 0.8904,
      "step": 494
    },
    {
      "epoch": 2.020408163265306,
      "grad_norm": 1.7406408786773682,
      "learning_rate": 0.00018169072164948456,
      "loss": 0.5436,
      "step": 495
    },
    {
      "epoch": 2.024489795918367,
      "grad_norm": 2.265122175216675,
      "learning_rate": 0.0001816494845360825,
      "loss": 0.703,
      "step": 496
    },
    {
      "epoch": 2.0285714285714285,
      "grad_norm": 2.259334087371826,
      "learning_rate": 0.00018160824742268042,
      "loss": 0.6699,
      "step": 497
    },
    {
      "epoch": 2.0326530612244897,
      "grad_norm": 2.2995729446411133,
      "learning_rate": 0.00018156701030927835,
      "loss": 0.6904,
      "step": 498
    },
    {
      "epoch": 2.036734693877551,
      "grad_norm": 2.362738847732544,
      "learning_rate": 0.00018152577319587628,
      "loss": 0.7572,
      "step": 499
    },
    {
      "epoch": 2.0408163265306123,
      "grad_norm": 1.75601065158844,
      "learning_rate": 0.00018148453608247424,
      "loss": 0.6479,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 4900,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.216440745818112e+17,
  "train_batch_size": 32,
  "trial_name": null,
  "trial_params": null
}
